---
title: "Week2"
author: "Michael Berger"
date: "10 November 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = '~/Studies/Coursera/10 - Capstone/corpus/en_US/')
library(readtext)
library(quanteda)
library(SnowballC)
library(ggplot2)
library(LaF)
library(dplyr)
library(tidyr)
library(plotly)
library(data.table)
library(hunspell)
library(sqldf)
```


```{r summaries, echo=FALSE, cache=T}

wc <- function(path)
{
    path = paste("'",path, "'", sep = "")
    w1 <- system(paste("wc -lwc",path), intern=TRUE)
    wsplit <- unlist(strsplit(w1," +"))
    wnum <-as.numeric(wsplit[2:4])
    return(data.frame('file' = wsplit[5], 'lines'=wnum[1], 'words'=wnum[2], 'chars'=wnum[3]))
}

folder = "/home/michael/Studies/Coursera/10-Capstone/corpus/en_US"

path <- paste(folder, currentFile, sep = "/")

filesinfo <- wc(path)

files = list.files(".", full.names = F)


lines = lapply(files, FUN = wc)# determine_nlines)

lala <- ldply(lines, rbind)

files <- data.frame(numlines = lines, stringsAsFactors = F)#, lines = as.vector(t(lns), mode = "any")

kable(files)
```


```{r}
enUS <- corpus(readtext('en_US.twitter.short.txt'))

enUSlines <- corpus_segment(enUS,pattern = "\n" )
docvars(enUSlines,'docnum')<- enUSlines$documents$`_segid`
rm( enUS)

#phrase <- "A computer once beat me at chess, but it was no match for me at kickboxing"
#phrase <- tokens(corpus("check out a movie"))
phrase <- tokens(corpus("rt"))
#cl <- textstat_collocations(phrase)
beat <- kwic(enUSlines,pattern = phrase, window = 50, case_insensitive = T, valuetype = "regex")
hasbeat <- beat[lengths(beat)>0]

bio <- enUSlines[15]
bio
texts
textstat_select(enUSlines, pattern = phrase,selection = "keep", valuetype = "fixed")
enUSlines[15]
```

```{r}
texts(enUSlines)[5]
```

```{r}
#enUS <- corpus(char_tolower(readNlines('en_US.twitter.txt', 40000)))

enUS <- corpus(char_tolower(get_lines('en_US.twitter.txt', 1:40000)))
lns <- get_lines('en_US.twitter.txt',1:2)# 40001:80000)
if (is.na(lns[1]))
  stop("shnitzl: couldn't read from file. probably path is wrong")
enUS80k <- corpus(char_tolower(get_lines('en_US.twitter.txt', 40001:80000))) 
head(texts(enUS80k))

profane <-char_tolower(get_lines('profanity_words.txt', 1:450))

# length(unique(profane))
tail(profane)
#enUSlines <- corpus_segment(enUS,pattern = "\n" )
#rm(enUS)
#save(enUSlines, file = "en_US.twitter.corpus.rds")

```

```{r}

```


#Search for keywords in context
```{r}
#what = " +(rt) +" # finds ReTweet tag. Don't really need it. Just do remove_tokens("rt")
# "(^|( +))(RT|rt)($| *:| +)" more advanced regex to find (ReTweet) tag. 
#what = "ã¶" #Finds non-english chars
#what =  "#\\w+" #Finds #hashtags
#phrase <- tokens(corpus(what), what = "word")

what = "_"
context <- kwic(Corp ,pattern = what, window = 50, case_insensitive = T, valuetype = "regex")

txts <- Corp[context$docname]
htags <- context$keyword
head(htags, 20)
head(txts)
```

```{r}
#words <- tokens(corpus(txts[1]))
words <- c("beer", "wiskey", "wine", "fsdngl","Taurus")
correct <- hunspell_check(words, dict = dictionary("en_US"))
print(correct)
```

```{r}
# this is a regex to find and substitute tokens which start with a number
withnums <- lvls[1:608]
#tail(lvls,40)

wn <- withnums[!grepl(pattern = "^(\\d+[,.]?)+", withnums)]
View(wn)

wihtoutnums <- gsub(pattern = "^(\\d+[,.]?)+", "", withnums)

View(cbind(withnums, wihtoutnums))
```



```{r}
# My attempt to create Twitter Hashtags segmentation algorithm. not finished 
source('~/Studies/Coursera/10 - Capstone/DSS_Capstone/Week2/hastags.r')
```

```{r echo=FALSE}
# with this you can check if there are some non-ascii chars remaining in dfm like chinese or russian 
rs <-colSums(Dfm1gr) # maybe its betteer to use docfreq()
rs <- data.frame(word = names(rs), freq = rs)
tf <- rs[order(rs$freq, decreasing = T ),]
lvls <- levels(rs$word)

cat(head(lvls,50), sep ="; ")
cat(tail(lvls,50), sep ="; ")
```


```{r}
features <- Dfm1gr@Dimnames$features
lvls <- features[order(features)]
cat(head(lvls,50), sep ="; ")
cat(tail(lvls,50), sep ="; ")
```


```{r}
enUStok <- tokens(enUS, what = "word", remove_numbers = T, remove_punct = T,
  remove_symbols = T, remove_separators = T,
  remove_twitter = T, remove_hyphens = T, remove_url = T,
  ngrams = 2:3)

```



```{r}
twitter_data <- stri_trans_general(twitter_data, "latin-ascii") #remove non-ASCII characters




toks <- tokens(enUS80k, what = "word", remove_numbers = T, remove_punct = T,
  remove_symbols = T, remove_separators = T,
  remove_twitter = T, remove_hyphens = T, remove_url = T)
toks <- tokens_remove(toks, c(stopwords("english"), profane))
bigrams <- tokens_ngrams(toks, 2)


```

```{r}
rs<-colSums(Dfmtoks80k)
rs <- data.frame(word = names(rs), freq = rs)
rso <- rs[order(rs, decreasing = T )]


# with this you can check if there are some non-ascii chars remaining in dfm like chinese or russian 
lvls <- levels(rs$word)
from = 10000
lvls[from:(from+100)]
```

```{r}

Dfmtoks80k <- dfm(toks, tolower = TRUE, stem = F) 
tf <- topfeatures(Dfmtoks80k,1000, scheme =  "docfreq")

tf <- data.frame(words = names(tf), freq = tf , stringsAsFactors = F)

Plot <- ggplot(head(tf,50), aes(x=reorder(words, freq), y=freq)) + 
        geom_bar(stat="Identity", fill='red') +
        coord_flip() #+ xlab(xlabel) + ylab(ylabel) + ggtitle(Gtitle)
Plot
```


```{r}
Dfm2grm80k <- dfm(bigrams, tolower = TRUE, stem = F) 
tf <- topfeatures(bigramsDfm,1000, scheme =  "docfreq")

tf <- data.frame(words = names(tf), freq = tf , stringsAsFactors = F)

Plot <- ggplot(head(tf,50), aes(x=reorder(words, freq), y=freq)) + 
        geom_bar(stat="Identity", fill='red') +
        coord_flip() #+ xlab(xlabel) + ylab(ylabel) + ggtitle(Gtitle)
Plot
```


```{r}
ngrams <- tokens_ngrams(toks, n = 3)
trigramsDfm <- dfm(ngrams, tolower = TRUE, stem = F) 
tf <- topfeatures(trigramsDfm,50, scheme =  "docfreq")

tf <- data.frame(words = names(tf), freq = tf , stringsAsFactors = F)

Plot <- ggplot(head(tf,50), aes(x=reorder(words, freq), y=freq)) + 
        geom_bar(stat="Identity", fill='red') +
        coord_flip() #+ xlab(xlabel) + ylab(ylabel) + ggtitle(Gtitle)
Plot
```


```{r}
ng <- rbind.dfm(Dfm2grm40k , Dfm2grm80k)
```

```{r}
# Bigram word combinations plot
#Dfm2 <- dfm(bigrams, tolower = TRUE, stem = F) # no stemming required i think
tf <- topfeatures(Dfm2gr ,2000, scheme =  "docfreq")

tf <- data.frame(words = names(tf), freq = tf , stringsAsFactors = F)

# tfm <- mutate(tfm, word1 = strsplit(words,"_")[[1,]], word2 = strsplit(words,"_")[[2,]])
# word1 = strsplit(tfm$words,"_")
# word1[1][]
# tfm$word1 <- lapply(tf$words, FUN = function(wd) {strsplit(wd,"_")[[1]]}) 
# tfm$word2 <- lapply(tf$words, function(wd) {strsplit(wd,"_")[[2]]}) 
tfm <- tf %>% separate(col=words, sep = "_", into =c("word1", "word2"),remove =F)
#g<- ggplot(tfm, aes(word2,word1)) + geom_raster(aes(fill = freq)) + scale_fill_brewer(1)
  #scale_fill_viridis_c(option = "inferno")

g<- ggplot(tfm, aes(x=word2,y=word1, text = words, col = freq)) + geom_point(size = 1) + 
   scale_fill_viridis_c(aesthetics = "col",option = "D", direction = -1) + theme_classic() + 
   theme(axis.text.x = element_text(angle = 90, hjust = 1))
gg<- ggplotly(g, tooltip = c("text","freq"))


length(unique(tfm$word1))
length(unique(tfm$word2))
```


```{r}

#An attempt to make a plot with less code using facet_grid. not really suitable for what I whant.  
tf <- topfeatures(Dfm1gr,30, scheme =  "docfreq")
tf1 <- data.frame(words = names(tf), freq = tf, gram = rep(1,length(tf)) , stringsAsFactors = F)

tf <- topfeatures(Dfm2gr,30, scheme =  "docfreq")
tf2 <- data.frame(words = names(tf), freq = tf , gram = rep(2,length(tf)) , stringsAsFactors = F)

tf <- topfeatures(Dfm3gr,30, scheme =  "docfreq")
tf3 <- data.frame(words = names(tf), freq = tf, gram = rep(3,length(tf))  , stringsAsFactors = F)

tf <- rbind(tf1, tf2,tf3)
                     
plt123grm <- ggplot(tf, aes(x=reorder(words, freq), y=freq)) + 
        geom_bar(stat="Identity", fill='red') + facet_grid(. ~ gram, scales = "free") +
        coord_flip() + xlab("1-grams") + ylab("Number of Apperarances") + ggtitle(" most frequent tokens") 

plt123grm

  
grid.arrange(plt1grm, plt2grm, plt3grm, ncol=3)

```

```{r}

ggplot(mlt, aes(x  = Var1, y = Var2)) + geom_raster(aes(fill = value), hjust=0.5,
vjust=0.5, interpolate=FALSE)
mlt <- melt(mt)
```



```{r fcm_plot}
if(F)
toks <- corpus_subset(enUS) %>%
    tokens(remove_punct = TRUE) %>%
    tokens_tolower() %>%
    tokens_remove(stopwords("english"), padding = FALSE)
myfcm <- fcm(toks, context = "window", tri = FALSE)
feat <- names(topfeatures(myfcm, 30))
fcm_select(myfcm, feat, verbose = FALSE) %>%
    textplot_network(min_freq = 0.5)
```



# should use benchmarking (https://github.com/hfoffani/dsci-benchmark)
# should use LaF package (https://rdrr.io/cran/LaF/man/)  (Fast Access to Large ASCII Files)



# get number of lines. there is a determine_nlines function in the LaF package and its 52 times faster.
```{r }
nlines <- function(filename )
{
  tic <- Sys.time()
  testcon <- file(filename,open="r")
  readsizeof <- 20000
  answer <- 0
  while((linesread <- length(readLines(testcon,readsizeof))) > 0 ) 
  {
    answer <- answer+linesread 
  }
  close(testcon)
  toc <- Sys.time() - tic
  print(toc)
  answer
}

nl <- nlines("en_US.twitter.txt")

tic <- Sys.time()
dnl <- determine_nlines("en_US.twitter.txt")
toc <- Sys.time() - tic
print(toc)
9.9/0.19

```

```{r}
timeit
```


```{r}


readNlines <- function(filename, N)
{
  tic <- Sys.time()
  #N <- 40000
  #filename <- "en_US.twitter.txt"
  conn <- file(filename,open="r")
  lines <- readLines(conn,N)
  close(conn)
  toc <- Sys.time() - tic
  toc
  lines
}
#test
t <-readNlines("en_US.twitter.txt", 13)
```

```{r}
readRangelines <- function(filename, from = 0 , to, num)
{
  #N <- 40000
  #filename <- "en_US.twitter.txt"
  conn <- file(filename,open="r")
  if(from!=0)
    devnull <- readLines(conn,from-1)
  if(missing(num)){num <- to-from}
  if(from > to) {stop("You think you are funny?")}
  lines <- readLines(conn,num)
  close(conn)
  lines
}
#test
readRangelines("en_US.twitter.txt",7,4)
```



#Model building

```{r}
load("~/Studies/Coursera/10-Capstone/corpus/en_US/123grams.rData")
```


```{r}
cS<-colSums(Dfm2gr)
bgfreq <- data.frame(words = names(cS), freq = cS , stringsAsFactors = F)
tfm <- bgfreq %>% separate(col=words, sep = "_", into =c("word1", "word2"),remove =T)
object.size(tfm)
```

```{r} 
#oops there is a problem: some tokens didn't get split beacuse some tokens have more than one underscore.
#reason: there were underscores in raw data, usually instead of a whitespace, so the tokenizer failed to split the words and I had word tokens like "look_tired". 
wrn <- c(46088, 46089, 64882, 83497, 100158, 115136, 118818, 118819, 126345, 126347, 161493, 161494, 171207, 171977, 171979, 242558, 258300, 352877, 372766, 372768)
cS[wrn]

```



```{r}
#POC: compressing the data using SQL indexing

#tfm is output of topFeatures() of bigrams 
#tfm <- subset(tfm, select = -c(words)) #I dont need words column, it's just word1_word2, should remove it even at an earlier step.

#words is a parent table of all the unique words in a corpus 
words <- (unique(c(tfm$word1, tfm$word2)))
words <- data.frame(id = 1:length(words) ,word = words, stringsAsFactors = F)

# check out what are you selecting
sel <- sqldf("select * from words, tfm where words.word = tfm.word1")

#find ids of each first word in parent table 
word1 <- sqldf("select id from  tfm, words where words.word = tfm.word1")
#find ids of each second word in parent table 
word2 <- sqldf("select id from  tfm, words where words.word = tfm.word2")

#Create table similar to tfm, but with word ids instead of words themselves.
bigrams <-data.frame(word1 = word1$id ,word2 = word2$id, freq = tfm$freq, stringsAsFactors = F)

#Reconstruct tfm again to check if it is equal to tfm 
check = sqldf("select w1.word as w1, w2.word as w2, bigrams.freq
               from bigrams join words w1 on (w1.id == bigrams.word1)
                            join words w2 on (w2.id == bigrams.word2) ")

colSums(check == tfm) # sums of all columns must be equal to tfm's length
dim(tfm)
#demonstrate that this approach really compresses the data 3 times.
object.size(tfm)/(object.size(words) + object.size(bigrams)) 

sqldf("attach 'mydb' as new") 
sqldf("create table bg as select * from bigrams", dbname = "mydb")
sqldf("create table wd as select * from words", dbname = "mydb")
sqldf("create table tfmdb as select * from tfm", dbname = "mydb")
sqldf("create table bi_grams as select * from bgfreq", dbname = "mydb")
sqldf("select * from bg", dbname = "mydb")
```


```{r}
# This is how I will search for bigram
tic <- Sys.time()
lal <- sqldf("select w1.word as w1, w2.word as w2, bigrams.freq
              from bigrams join words w1 on (w1.id == bigrams.word1)
                           join words w2 on (w2.id == bigrams.word2) 
              where w1 ='year' and w2 like 'ago'")
toc <- Sys.time()-tic
print(toc)
lal
```

```{r}
# This is how I will search for bigram
tic <- Sys.time()
  lal <- sqldf("select w1.word as w1, w2.word as w2, bg.freq
                from bg join wd w1 on (w1.id == bg.word1)
                             join wd w2 on (w2.id == bg.word2) 
                where w1 ='year' and w2 like 'ago'", dbname = "mydb")
toc <- Sys.time()-tic
print(toc)
lal
```

```{r}
# This is how I will search for bigram
tic <- Sys.time()
lal <- sqldf("select  w1.word as wd1, w2.word as wd2, bg.freq
              from bg join wd w1 on ( bg.word1 = w1.id)
                      join wd w2 on ( bg.word2 = w2.id ) 
              where wd1 ='hello' and wd2 = 'darlin' ", dbname = "mydb")
toc <- Sys.time()-tic
toc
print(lal)
```

```{r}
# This is how I will search for bigram
tic <- Sys.time()
lal <- sqldf("select * from tfmdb 
             where word2 = 'fit'", dbname = "mydb") 
      #        where freq = 345", dbname = "mydb")

       #       where word1 ='year'", dbname = "mydb")
toc <- Sys.time()-tic
print(toc)
lal
```


```{r}
# This is how I will search for bigram
tic <- Sys.time()
lal <- sqldf("select * from bi_grams 
             where words like '%\\_fit' escape '\\'", dbname = "mydb") 
      #        where freq = 345", dbname = "mydb")

       #       where word1 ='year'", dbname = "mydb")
toc <- Sys.time()-tic
print(toc)
dim(lal)
lal
```


```{r}
# This is how I will search for bigram
tic <- Sys.time()
lal <- sqldf(c("update bi_grams set freq = freq + inc.freq where words = (select * from bi_grams where words like '%\\_fit' escape '\\') inc ", 
              "select * from bi_grams where words like '%\\_fit' escape '\\'"), dbname = "mydb") 
      #        where freq = 345", dbname = "mydb")

       #       where word1 ='year'", dbname = "mydb")
toc <- Sys.time()-tic
print(toc)
dim(lal)
lal
```



```{r}

words <- unique(c(tfm$word1, tfm$word2))
words <- data.frame(id = 1:length(words) ,word = words, stringsAsFactors = F)
words[words$word==c("years","even","ago"),]


word2 <- words$word %in% c("years","even","ago")

```

















