---
title: "Week2"
author: "Michael Berger"
date: "10 November 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
  knitr::opts_knit$set(root.dir = '~/Studies/Coursera/10-Capstone/DSS_Capstone/Week2/')
data.dir <<- "/home/michael/Studies/Coursera/10-Capstone/corpus/en_US/"
dbname   <<- "/home/michael/Studies/Coursera/10-Capstone/corpus/en_US/ngrams"
library(readtext)
library(quanteda)
library(SnowballC)
library(ggplot2)
library(LaF)
library(dplyr)
library(tidyr)
library(plotly)
library(data.table)
library(hunspell)
library(sqldf)
library(stringi)
```


```{r summaries, echo=FALSE, cache=T}

wc <- function(path)
{
    path = paste("'",path, "'", sep = "")
    w1 <- system(paste("wc -lwc",path), intern=TRUE)
    wsplit <- unlist(strsplit(w1," +"))
    wnum <-as.numeric(wsplit[2:4])
    return(data.frame('file' = wsplit[5], 'lines'=wnum[1], 'words'=wnum[2], 'chars'=wnum[3]))
}

folder = "/home/michael/Studies/Coursera/10-Capstone/corpus/en_US"

path <- paste(folder, currentFile, sep = "/")

filesinfo <- wc(path)

files = list.files(".", full.names = F)


lines = lapply(files, FUN = wc)# determine_nlines)

lala <- ldply(lines, rbind)

files <- data.frame(numlines = lines, stringsAsFactors = F)#, lines = as.vector(t(lns), mode = "any")

kable(files)
```


```{r}
enUS <- corpus(readtext('en_US.twitter.short.txt'))

enUSlines <- corpus_segment(enUS,pattern = "\n" )
docvars(enUSlines,'docnum')<- enUSlines$documents$`_segid`
rm( enUS)

#phrase <- "A computer once beat me at chess, but it was no match for me at kickboxing"
#phrase <- tokens(corpus("check out a movie"))
phrase <- tokens(corpus("rt"))
#cl <- textstat_collocations(phrase)
beat <- kwic(enUSlines,pattern = phrase, window = 50, case_insensitive = T, valuetype = "regex")
hasbeat <- beat[lengths(beat)>0]

bio <- enUSlines[15]
bio
texts
textstat_select(enUSlines, pattern = phrase,selection = "keep", valuetype = "fixed")
enUSlines[15]
```

```{r}
texts(enUSlines)[5]
```

```{r}
#enUS <- corpus(char_tolower(readNlines('en_US.twitter.txt', 40000)))

enUS <- corpus(char_tolower(get_lines('en_US.twitter.txt', 1:40000)))
lns <- get_lines('en_US.twitter.txt',1:2)# 40001:80000)
if (is.na(lns[1]))
  stop("shnitzl: couldn't read from file. probably path is wrong")
enUS80k <- corpus(char_tolower(get_lines('en_US.twitter.txt', 40001:80000))) 
head(texts(enUS80k))

profane <-char_tolower(get_lines('profanity_words.txt', 1:450))

# length(unique(profane))
tail(profane)
#enUSlines <- corpus_segment(enUS,pattern = "\n" )
#rm(enUS)
#save(enUSlines, file = "en_US.twitter.corpus.rds")

```

```{r}

```


#Search for keywords in context
```{r}
#what = " +(rt) +" # finds ReTweet tag. Don't really need it. Just do remove_tokens("rt")
# "(^|( +))(RT|rt)($| *:| +)" more advanced regex to find (ReTweet) tag. 
#what = "ã¶" #Finds non-english chars
#what =  "#\\w+" #Finds #hashtags
#phrase <- tokens(corpus(what), what = "word")

what = "_"
context <- kwic(Corp ,pattern = what, window = 50, case_insensitive = T, valuetype = "regex")

txts <- Corp[context$docname]
htags <- context$keyword
head(htags, 20)
head(txts)
```

```{r}
#words <- tokens(corpus(txts[1]))
words <- c("beer", "wiskey", "wine", "fsdngl","Taurus")
correct <- hunspell_check(words, dict = dictionary("en_US"))
print(correct)
```

```{r}
# this is a regex to find and substitute tokens which start with a number
withnums <- lvls[1:608]
#tail(lvls,40)

wn <- withnums[!grepl(pattern = "^(\\d+[,.]?)+", withnums)]
View(wn)

wihtoutnums <- gsub(pattern = "^(\\d+[,.]?)+", "", withnums)

View(cbind(withnums, wihtoutnums))
```



```{r}
# My attempt to create Twitter Hashtags segmentation algorithm. not finished 
source('~/Studies/Coursera/10 - Capstone/DSS_Capstone/Week2/hastags.r')
```

```{r echo=FALSE}
# with this you can check if there are some non-ascii chars remaining in dfm like chinese or russian 
rs <-colSums(Dfm1gr) # maybe its betteer to use docfreq()
rs <- data.frame(word = names(rs), freq = rs)
tf <- rs[order(rs$freq, decreasing = T ),]
lvls <- levels(rs$word)

cat(head(lvls,50), sep ="; ")
cat(tail(lvls,50), sep ="; ")
```


```{r}
features <- Dfm1gr@Dimnames$features
lvls <- features[order(features)]
cat(head(lvls,50), sep ="; ")
cat(tail(lvls,50), sep ="; ")
```


```{r}
enUStok <- tokens(enUS, what = "word", remove_numbers = T, remove_punct = T,
  remove_symbols = T, remove_separators = T,
  remove_twitter = T, remove_hyphens = T, remove_url = T,
  ngrams = 2:3)

```



```{r}
twitter_data <- stri_trans_general(twitter_data, "latin-ascii") #remove non-ASCII characters




toks <- tokens(enUS80k, what = "word", remove_numbers = T, remove_punct = T,
  remove_symbols = T, remove_separators = T,
  remove_twitter = T, remove_hyphens = T, remove_url = T)
toks <- tokens_remove(toks, c(stopwords("english"), profane))
bigrams <- tokens_ngrams(toks, 2)


```

```{r}
rs<-colSums(Dfmtoks80k)
rs <- data.frame(word = names(rs), freq = rs)
rso <- rs[order(rs, decreasing = T )]


# with this you can check if there are some non-ascii chars remaining in dfm like chinese or russian 
lvls <- levels(rs$word)
from = 10000
lvls[from:(from+100)]
```

```{r}

Dfmtoks80k <- dfm(toks, tolower = TRUE, stem = F) 
tf <- topfeatures(Dfmtoks80k,1000, scheme =  "docfreq")

tf <- data.frame(words = names(tf), freq = tf , stringsAsFactors = F)

Plot <- ggplot(head(tf,50), aes(x=reorder(words, freq), y=freq)) + 
        geom_bar(stat="Identity", fill='red') +
        coord_flip() #+ xlab(xlabel) + ylab(ylabel) + ggtitle(Gtitle)
Plot
```


```{r}
Dfm2grm80k <- dfm(bigrams, tolower = TRUE, stem = F) 
tf <- topfeatures(bigramsDfm,1000, scheme =  "docfreq")

tf <- data.frame(words = names(tf), freq = tf , stringsAsFactors = F)

Plot <- ggplot(head(tf,50), aes(x=reorder(words, freq), y=freq)) + 
        geom_bar(stat="Identity", fill='red') +
        coord_flip() #+ xlab(xlabel) + ylab(ylabel) + ggtitle(Gtitle)
Plot
```


```{r}
ngrams <- tokens_ngrams(toks, n = 3)
trigramsDfm <- dfm(ngrams, tolower = TRUE, stem = F) 
tf <- topfeatures(trigramsDfm,50, scheme =  "docfreq")

tf <- data.frame(words = names(tf), freq = tf , stringsAsFactors = F)

Plot <- ggplot(head(tf,50), aes(x=reorder(words, freq), y=freq)) + 
        geom_bar(stat="Identity", fill='red') +
        coord_flip() #+ xlab(xlabel) + ylab(ylabel) + ggtitle(Gtitle)
Plot
```


```{r}
ng <- rbind.dfm(Dfm2grm40k , Dfm2grm80k)
```

```{r}
# Bigram word combinations plot
#Dfm2 <- dfm(bigrams, tolower = TRUE, stem = F) # no stemming required i think
tf <- topfeatures(Dfm2gr ,2000, scheme =  "docfreq")

tf <- data.frame(words = names(tf), freq = tf , stringsAsFactors = F)

# tfm <- mutate(tfm, word1 = strsplit(words,"_")[[1,]], word2 = strsplit(words,"_")[[2,]])
# word1 = strsplit(tfm$words,"_")
# word1[1][]
# tfm$word1 <- lapply(tf$words, FUN = function(wd) {strsplit(wd,"_")[[1]]}) 
# tfm$word2 <- lapply(tf$words, function(wd) {strsplit(wd,"_")[[2]]}) 
tfm <- tf %>% separate(col=words, sep = "_", into =c("word1", "word2"),remove =F)
#g<- ggplot(tfm, aes(word2,word1)) + geom_raster(aes(fill = freq)) + scale_fill_brewer(1)
  #scale_fill_viridis_c(option = "inferno")

g<- ggplot(tfm, aes(x=word2,y=word1, text = words, col = freq)) + geom_point(size = 1) + 
   scale_fill_viridis_c(aesthetics = "col",option = "D", direction = -1) + theme_classic() + 
   theme(axis.text.x = element_text(angle = 90, hjust = 1))
gg<- ggplotly(g, tooltip = c("text","freq"))


length(unique(tfm$word1))
length(unique(tfm$word2))
```


```{r}

#An attempt to make a plot with less code using facet_grid. not really suitable for what I whant.  
tf <- topfeatures(Dfm1gr,30, scheme =  "docfreq")
tf1 <- data.frame(words = names(tf), freq = tf, gram = rep(1,length(tf)) , stringsAsFactors = F)

tf <- topfeatures(Dfm2gr,30, scheme =  "docfreq")
tf2 <- data.frame(words = names(tf), freq = tf , gram = rep(2,length(tf)) , stringsAsFactors = F)

tf <- topfeatures(Dfm3gr,30, scheme =  "docfreq")
tf3 <- data.frame(words = names(tf), freq = tf, gram = rep(3,length(tf))  , stringsAsFactors = F)

tf <- rbind(tf1, tf2,tf3)
                     
plt123grm <- ggplot(tf, aes(x=reorder(words, freq), y=freq)) + 
        geom_bar(stat="Identity", fill='red') + facet_grid(. ~ gram, scales = "free") +
        coord_flip() + xlab("1-grams") + ylab("Number of Apperarances") + ggtitle(" most frequent tokens") 

plt123grm

  
grid.arrange(plt1grm, plt2grm, plt3grm, ncol=3)

```

```{r}

ggplot(mlt, aes(x  = Var1, y = Var2)) + geom_raster(aes(fill = value), hjust=0.5,
vjust=0.5, interpolate=FALSE)
mlt <- melt(mt)
```



```{r fcm_plot}
if(F)
toks <- corpus_subset(enUS) %>%
    tokens(remove_punct = TRUE) %>%
    tokens_tolower() %>%
    tokens_remove(stopwords("english"), padding = FALSE)
myfcm <- fcm(toks, context = "window", tri = FALSE)
feat <- names(topfeatures(myfcm, 30))
fcm_select(myfcm, feat, verbose = FALSE) %>%
    textplot_network(min_freq = 0.5)
```



# should use benchmarking (https://github.com/hfoffani/dsci-benchmark)
# should use LaF package (https://rdrr.io/cran/LaF/man/)  (Fast Access to Large ASCII Files)



# get number of lines. there is a determine_nlines function in the LaF package and its 52 times faster.
```{r }
nlines <- function(filename )
{
  tic <- Sys.time()
  testcon <- file(filename,open="r")
  readsizeof <- 20000
  answer <- 0
  while((linesread <- length(readLines(testcon,readsizeof))) > 0 ) 
  {
    answer <- answer+linesread 
  }
  close(testcon)
  toc <- Sys.time() - tic
  print(toc)
  answer
}

nl <- nlines("en_US.twitter.txt")

tic <- Sys.time()
dnl <- determine_nlines("en_US.twitter.txt")
toc <- Sys.time() - tic
print(toc)
9.9/0.19

```

```{r}
timeit
```


```{r}


readNlines <- function(filename, N)
{
  tic <- Sys.time()
  #N <- 40000
  #filename <- "en_US.twitter.txt"
  conn <- file(filename,open="r")
  lines <- readLines(conn,N)
  close(conn)
  toc <- Sys.time() - tic
  toc
  lines
}
#test
t <-readNlines("en_US.twitter.txt", 13)
```

```{r}
readRangelines <- function(filename, from = 0 , to, num)
{
  #N <- 40000
  #filename <- "en_US.twitter.txt"
  conn <- file(filename,open="r")
  if(from!=0)
    devnull <- readLines(conn,from-1)
  if(missing(num)){num <- to-from}
  if(from > to) {stop("You think you are funny?")}
  lines <- readLines(conn,num)
  close(conn)
  lines
}
#test
readRangelines("en_US.twitter.txt",7,4)
```



#Model building

```{r}
load("~/Studies/Coursera/10-Capstone/corpus/en_US/123grams.rData")
```


```{r}

```

```{r} 
#oops there is a problem: some tokens didn't get split beacuse some tokens have more than one underscore.
#reason: there were underscores in raw data, usually instead of a whitespace, so the tokenizer failed to split the words and I had word tokens like "look_tired". 
wrn <- c(46088, 46089, 64882, 83497, 100158, 115136, 118818, 118819, 126345, 126347, 161493, 161494, 171207, 171977, 171979, 242558, 258300, 352877, 372766, 372768)
cS[wrn]

```



```{r}
#POC: compressing the data using SQL indexing

#tfm is output of topFeatures() colSums() of bigrams 
#tfm <- subset(tfm, select = -c(words)) #I dont need words column, it's just word1_word2, should remove it even at an earlier step.

cS<-colSums(Dfm2gr)
bgfreq <- data.frame(words = names(cS), freq = cS , stringsAsFactors = F)
tfm <- bgfreq %>% separate(col=words, sep = "_", into =c("word1", "word2"),remove =T)
object.size(tfm)

#words is a parent table of all the unique words in a corpus 
words <- (unique(c(tfm$word1, tfm$word2)))
words <- data.frame(id = 1:length(words) ,word = words, stringsAsFactors = F)

# check out what are you selecting
sel <- sqldf("select * from words, tfm where words.word = tfm.word1")

#find ids of each first word in parent table 
word1 <- sqldf("select id from  tfm, words where words.word = tfm.word1")
#find ids of each second word in parent table 
word2 <- sqldf("select id from  tfm, words where words.word = tfm.word2")

#Create table similar to tfm, but with word ids instead of words themselves.
bigrams <-data.frame(word1 = word1$id ,word2 = word2$id, freq = tfm$freq, stringsAsFactors = F)

#Reconstruct tfm again to check if it is equal to tfm 
  check = sqldf("select w1.word as w1, w2.word as w2, bigrams.freq
                 from bigrams join words w1 on (w1.id == bigrams.word1)
                              join words w2 on (w2.id == bigrams.word2) ")

colSums(check == tfm) # sums of all columns must be equal to tfm's length
dim(tfm)
#demonstrate that this approach really compresses the data 3 times.
object.size(tfm)/(object.size(words) + object.size(bigrams)) 

sqldf("attach 'mydb' as new") 
sqldf("create table bg as select * from bigrams", dbname = "mydb")
sqldf("create table wd as select * from words", dbname = "mydb")
sqldf("create table tfmdb as select * from tfm", dbname = "mydb")
sqldf("create table bi_grams as select * from bgfreq", dbname = "mydb")
sqldf("select * from bg", dbname = "mydb")
```


```{r}
# This is how I will search for bigram
tic <- Sys.time()
lal <- sqldf("select w1.word as w1, w2.word as w2, bigrams.freq
              from bigrams join words w1 on (w1.id == bigrams.word1)
                           join words w2 on (w2.id == bigrams.word2) 
              where w1 ='year' and w2 like 'ago'")
toc <- Sys.time()-tic
print(toc)
lal
```

```{r}
# This is how I will search for bigram
tic <- Sys.time()
  lal <- sqldf("select w1.word as w1, w2.word as w2, bg.freq
                from bg join wd w1 on (w1.id == bg.word1)
                             join wd w2 on (w2.id == bg.word2) 
                where w1 ='year' and w2 like 'ago'", dbname = "mydb")
toc <- Sys.time()-tic
print(toc)
lal
```

```{r}
# This is how I will search for bigram
tic <- Sys.time()
lal <- sqldf("select  w1.word as wd1, w2.word as wd2, bg.freq
              from bg join wd w1 on ( bg.word1 = w1.id)
                      join wd w2 on ( bg.word2 = w2.id ) 
              where wd1 ='hello' and wd2 = 'darlin' ", dbname = "mydb")
toc <- Sys.time()-tic
toc
print(lal)
```

```{r}
# This is how I will search for bigram
tic <- Sys.time()
lal <- sqldf("select * from tfmdb 
             where word2 = 'fit'", dbname = "mydb") 
      #        where freq = 345", dbname = "mydb")

       #       where word1 ='year'", dbname = "mydb")
toc <- Sys.time()-tic
print(toc)
lal
```


```{r}
# This is how I will search for bigram
tic <- Sys.time()
lal <- sqldf("select * from bi_grams 
             where words like '%\\_fit' escape '\\'", dbname = "mydb") 
      #        where freq = 345", dbname = "mydb")

       #       where word1 ='year'", dbname = "mydb")
toc <- Sys.time()-tic
print(toc)
dim(lal)
lal
```


```{r}
# This is how I will search for bigram
library(profvis)
profvis({
tic <- Sys.time()
lal <- sqldf("select * from gr3
             where words like 'hello_dear\\_%' escape '\\'") 
      #        where freq = 345", dbname = "mydb")

       #       where word1 ='year'", dbname = "mydb")
toc <- Sys.time()-tic})
print(toc)
dim(lal)
lal
```



```{r}
sqldf("attach 'ngrams' as new") 
gr1 <- FreqNgr$gr1
sqldf("create table gram1 as select * from gr1 ", dbname = "ngrams")
gr2 <- FreqNgr$gr2
sqldf("create table gram2 as select * from gr2 ", dbname = "ngrams")
gr3 <- FreqNgr$gr3
sqldf("create table gram3 as select * from gr3 ", dbname = "ngrams")
gr4 <- FreqNgr$gr4
sqldf("create table gram4 as select * from gr4 ", dbname = "ngrams")
gr5 <- FreqNgr$gr5
sqldf("create table gram5 as select * from gr5 ", dbname = "ngrams")



# sqldf("create table wd as select * from words", dbname = "mydb")
# sqldf("create table tfmdb as select * from tfm", dbname = "mydb")
# sqldf("create table bi_grams as select * from bgfreq", dbname = "mydb")
# sqldf("select * from bg", dbname = "mydb")
```


```{r}
# This is how I will search for bigram
tic <- Sys.time()
lal <- sqldf(c("update bi_grams set freq = freq + inc.freq where words = (select * from bi_grams where words like '%\\_fit' escape '\\') inc ", 
              "select * from bi_grams where words like '%\\_fit' escape '\\'"), dbname = "mydb") 
      #        where freq = 345", dbname = "mydb")

       #       where word1 ='year'", dbname = "mydb")
toc <- Sys.time()-tic
print(toc)
dim(lal)
lal
```



```{r}

words <- unique(c(tfm$word1, tfm$word2))
words <- data.frame(id = 1:length(words) ,word = words, stringsAsFactors = F)
words[words$word==c("years","even","ago"),]


word2 <- words$word %in% c("years","even","ago")

```







 
```{r sample_and_freq, comment=NA}
l <- determine_nlines('profanity_words.txt') #read profanity vocabulary and convert to lower
profane <-char_tolower(get_lines('profanity_words.txt', 1:l))
profane <- profane[!is.na(profane)]

sample_and_freq <- function(filename ,line_numbers)
{
  tic <- Sys.time()
  if (is.na(get_lines(filename,1:2)[1])) # check that I can read from file
  {
    print(paste("can't read from file", filename))
    stop()
  }

  Lines <- get_lines(filename, line_numbers) #sample some lines from a file
  Encoding(Lines) <- "latin1"  #remove non-ascii chars 
  Lines <- iconv(Lines, "latin1", "ASCII", sub="")
  Lines <- gsub('_+', ' ', Lines, perl=T) #replace all underscores (including multiple _____) with whitespace
  Corp <- corpus(char_tolower(Lines)) # everything to lowerCase and create corpus
  
  Toks<- tokens(Corp, what = "word", remove_numbers = T, remove_punct = T, remove_symbols = T,
  remove_separators = T, remove_twitter = T, remove_hyphens = T, remove_url = T)
  
  Toks<- tokens_remove(Toks, c(stopwords("english"), profane, "rt"))  # also remove ReTweet tag
  
  bigrams <- tokens_ngrams(Toks, 2)
  Dfm2gr <- dfm(bigrams, tolower = TRUE, stem = F)
  cS<-sort(colSums(Dfm2gr), decreasing = T)
  Freq2gr <- data.table(words = names(cS), freq = cS , stringsAsFactors = F, key = "words")
  print(paste(line_numbers[1], Sys.time()-tic), sep = "| ")
  return(Freq2gr) #(cS)#
}
```

```{r merge_2grams_only}
path <- "/home/michael/Studies/Coursera/10-Capstone/corpus/en_US/"
currFile <- 'en_US.blogs.txt'

sampSize = 1e3 #1e4 # 
nslices = floor(files[currFile,]$lines / sampSize) #upper boundary for processing loop
rng = 890:nslices # 10 #
Freq1gr <- data.table(words = character(), freq = numeric(), stringsAsFactors = F, key = "words")
toc <- numeric()

stoks <- numeric() #num of toks at each iteration 
ttoks<- numeric() #num of toks in merged data.frame


tic <- Sys.time()
for (i  in rng) # 900)#
{

  lnums <- (i*sampSize+1):((i+1)*sampSize)
  c(head(lnums),'...',tail(lnums))
  Freq1gr2 <- sample_and_freq(paste(path,  currFile, sep = ''), lnums)
  stoks <- c(stoks, dim(Freq1gr2)[1])
   
  Freq1gr<-merge(Freq1gr,Freq1gr2, by.x = "words", by.y = "words", all =T)
  Freq1gr[is.na(Freq1gr)] <-0 #probably should do it with only numeric columns for speed
  Freq1gr <- mutate(Freq1gr, freq = freq.x + freq.y)
  Freq1gr <- Freq1gr[c("words","freq")]
  ttoks <- c(ttoks, dim(Freq1gr)[1])
  toc <- c(toc, Sys.time())
  #print(Sys.time() - tic)
}


tod <- toc- toc[1]
todd <- tod[-length(tod)]
plot(tod[-1] - todd)

g <- ggplot() + geom_line(aes(x = rng, y = cumsum(stoks)) , color = "red") +
                geom_line(aes(x = rng, y = ttoks), color = "blue" ) 
g

object.size(Freq1gr)

Freq1gr <- Freq1gr[order(Freq1gr$freq , decreasing = T  ),]

#save(Freq1gr, toc, stoks, ttoks, file = "Freq2gr.rData")

# j = 1050
# Freq1gr[j:(j+50),]
# 
# plot(head(Freq1gr$freq, 4000))
# 
# sum(Freq1gr$freq >5)
# sum(Freq1gr$freq >4)
View(cbind(tod,todd))


```


```{r}

prof = NULL
#prof <- profvis({

#User Parameters:
path <- "/home/michael/Studies/Coursera/10-Capstone/corpus/en_US/"
currFile <- 'en_US.twitter.txt'
sampSize = 5e4 #1e4 # hom much lines to sample in a chunk
ngrO = 1 #:5 # what Orders of ngrams you require

tic <- Sys.time()

  #Derived parameters
  filename <- paste(path,  currFile, sep = '')
  nslices = floor(files[currFile,]$lines / sampSize) #upper boundary for processing loop
  rng = (nslices-1):nslices # 10 # for quick tests 
  #rng = 0:nslices # 10 # for full file run. must start at zero
  #A list of data.tables each of which holds ngrams of its order
  a <- list(data.table(word = character(), freq = numeric(), stringsAsFactors = F, key = "word"))
  FreqNgr <- rep(a, length(ngrO))
  
  
  #cl <- makeSOCKcluster(rep("localhost", length(ngO)))

  for (i  in rng) # 900)#
  {
    lnums <- (i*sampSize+1):((i+1)*sampSize)
    Toks <- sample_and_toks(filename, lnums)
    FreqNgr <- lapply(ngrO, ngramize, Toks, FreqNgr) #without use of parallel framework
    #FreqNgr <- parLapply(cl, ngO, ngramize, Toks, FreqNgr)
  }
  #stopCluster(cl)
  #}, prof_output = ".")
  names(FreqNgr) <- paste("gr",ngrO,sep = '') #stamp a names such as 'gr2', 'gr3' etc to each data.table in a list 
  
  #Freq1grBlogs <- FreqNgr
  #
  #FreqNgr["gr2"]
  #profvis(prof_input = "/home/michael/Studies/Coursera/10-Capstone/corpus/en_US/file4fce8ccb11c.Rprof")

save(FreqNgrBlogs,prof, file = paste(path, "ngrams.", currFile, "full.rData", sep = ''))

```


```{r sample_and_toks, comment=NA}
l <- determine_nlines('profanity_words.txt') #read profanity vocabulary and convert to lower
profane <-char_tolower(get_lines('profanity_words.txt', 1:l))
profane <- profane[!is.na(profane)]

load(file = 'filesInfo.rData') #load information about files: specifically number of lines 

sample_and_toks <- function(filename ,line_numbers)
{

  if (is.na(get_lines(filename,1:2)[1])) # check that I can read from file
  {
    print(paste("can't read from file", filename))
    stop()
  }

  Lines <- get_lines(filename, line_numbers) #sample some lines from a file
  Encoding(Lines) <- "latin1"  #remove non-ascii chars 
  Lines <- iconv(Lines, "latin1", "ASCII", sub="")
  Lines <- gsub('_+', ' ', Lines, perl=T) #replace all underscores (including multiple _____) with whitespace
  Corp <- corpus(char_tolower(Lines)) # everything to lowerCase and create corpus
  
  Toks<- tokens(Corp, what = "word", remove_numbers = T, remove_punct = T, remove_symbols = T,
  remove_separators = T, remove_twitter = T, remove_hyphens = T, remove_url = T)
  rm(Lines,Corp)
  Toks <- tokens_remove(Toks, c(stopwords("english"), profane, "rt"))# also remove ReTweet tag

  return(Toks) #(cS)# 
}  

ngramize <-function (n, Toks, FreqBig)
{
  library(quanteda)
  library(data.table)
  library(dplyr)
  ngrams <- tokens_ngrams(Toks, n)
  DfmNgr <- dfm(ngrams, tolower = F)
  cS<-sort(colSums(DfmNgr), decreasing = T)
  FreqThis <- data.table(word = names(cS), freq = cS , stringsAsFactors = F, key = "word")
  #FreqThis <- FreqNgr1$gr2

  wordCols <- paste("word", 1:n, sep = '')
  FreqThis <- FreqThis %>% separate(col=word, sep = "_", into = wordCols ,remove =T)
  
  
  #find ids of each first word in parent table 
  # word1 <- sqldf("select Id, word from words, FreqThis where (words.word == FreqThis.word1)", dbname = "ngrams")
  # FreqThis$word1 <- word1$Id
  # 
  # word2 <- sqldf("select Id, word from words, FreqThis where (words.word == FreqThis.word2)", dbname = "ngrams")
  # FreqThis$word2 <- word2$Id
  
  for (i in 1:n) #same as above, find ids of each first word in parent table, just for any n
  {
    wordi <- sqldf(paste("select Id, word from words, FreqThis where (words.word == FreqThis.word",i, ")", 
                           sep = ""), dbname ="ngrams")
    FreqThis[wordCols[i]] <- wordi$Id
  }
  
  
  
  # FreqBig<-merge(FreqBig[[n]],FreqThis, by.x = wordCols, by.y = wordCols, all =T, sort = T)
  # FreqBig[is.na(FreqBig)] <-0 #probably should do it with only numeric columns for speed
  # FreqBig <- mutate(FreqBig, freq = freq.x + freq.y)
  
  # return(FreqBig[c("word","freq")])
  return(FreqThis)
}


DBinsert <- function(FreqNgr, tables)
{
  ques <- c(paste("insert into ", tables$big ,"  select * from FreqNgr", sep = " "),
          paste("insert into ", tables$small ," (word1,word2, freq) 
                         select  word1,word2, sum(freq) as freq
                         from ", tables$big ,"
                         group by word1,word2", sep = " ")
        )
  sqldf(ques[2], dbname = "ngrams")
  return(swap(tables))
  
}

swap <- function(table)
{
  if (length(table)!=2) stop("tables length must be precisely 2!")
  tmp <- table[1]
  table[1] <- table[2]
  table[2] <- tmp
  return(table)
}

#prof = NULL
#prof <- profvis({

#User Parameters:
path <- "/home/michael/Studies/Coursera/10-Capstone/corpus/en_US/"
#currFile <- 'en_US.blogs.txt'
sampSize = 5e4  #1e4 # how much lines to sample in a chunk
ngrO = 1 #:5 # what Orders of ngrams you require

ngrams <- function(path, sampSize = 5e4, ngrO = 1, env = "test" )
{
  tic <- Sys.time()
  obs <- numeric()
  elapsed <- Sys.time()
  
  #A list of data.tables each of which holds ngrams of its order
  # a <- list(data.table(word = numeric(1:3), freq = numeric(), stringsAsFactors = F, key = "word"))
  # FreqNgr <- rep(a, length(ngrO))
  
  wordCols <- paste("word", 1:ngrO, sep = '')
  a <- data.table(freq = numeric()) 
  a[,(wordCols) := numeric(ngrO), by=freq]
  # FreqNgr <- list(a) # for compatibility I'll leave it as list of data.tables, but only one d.t will be there
  # names(FreqNgr) <- paste("gr",ngrO,sep = '') #stamp a names such as 'gr2', 'gr3' etc to each data.table in a list 
  FreqNgr <- a
  
  tables <- list(big = "gram2", small = "tmp")
  
  for (currFile in files[-4,]$file)
  {
    #Derived parameters
    filename <- paste(path,  currFile, sep = '')
    
    sampSize <- ifelse(env=="test", 1e2, sampSize )
    nslices = floor(files[currFile,]$lines / sampSize) #upper boundary for processing loop
    #rng = (nslices-1):nslices # 10 # for quick tests 
    #rng = 0:nslices # 10 # for full file run. must start at zero
    
    rndslices <- ceiling(runif(1,0,nslices)) # smaple random place in data for testing
    rng <- if(env=="test") (rndslices-1):rndslices else 0:nslices 
    
    
    #cl <- makeSOCKcluster(rep("localhost", length(ngO)))
  
    for (i  in rng) # 900)#
    {
      lnums <- (i*sampSize+1):((i+1)*sampSize)
      
      toc <-difftime(Sys.time(), tic, units = "mins")
      toc <- paste("| ", sprintf("%.1f", toc ), " ", attr(toc, "units"), "elapsed")
                                                  #56 is lenth of path
      print(paste("processing file ", substr(filename,56,nchar(filename)), "  lines ", 
        lnums[1], " - ", tail(lnums,1),toc))
      
      Toks <- sample_and_toks(filename, lnums)
      FreqNgr <- ngramize(ngrO, Toks, FreqNgr)
      rm(Toks)
      
      tables <- DBinsert(FreqNgr, tables)
      
      #FreqNgr <- parLapply(cl, ngO, ngramize, Toks, FreqNgr)
     
      ob <-dim(FreqNgr)[1]
      obs <- c(obs, ob)
      print(paste(ob, "unique tokens| ", object.size(FreqNgr), " bytes"))
      elapsed <- c(elapsed, difftime(Sys.time(), tic, units = "mins"))
    }
    #stopCluster(cl)
    #}, prof_output = ".")
   
    
    #Freq1grBlogs <- FreqNgr
    #
    #FreqNgr["gr2"]
    #profvis(prof_input = "/home/michael/Studies/Coursera/10-Capstone/corpus/en_US/file4fce8ccb11c.Rprof")
  
  }
  save(FreqNgr,obs, file = paste(path, ngrO, "grams." ,env, ".allFiles.rData", sep = ''))
  return(list(FreqNgr,obs))
}

# tmp <- ngrams(path, sampSize = 5e2, ngrO = 2, env = "test" )
# FreqNgr <- tmp[[1]]
# obs <- tmp[[2]]
# plot(obs, type = 'l')

```


## attempt to update the grams by sql sum on groupby

```{r, message=F}

#sqldf(c("drop table gram2","drop table tmp"), dbname = "ngrams")

sqldf("create table gram2 (word1 INT,word2 INT,freq REAL)", dbname = "ngrams")
sqldf("create table tmp (word1 INT,word2 INT,freq REAL)", dbname = "ngrams")

sqldf("insert into gram2  select * from grA", dbname = "ngrams")

sqldf("insert into gram2  select * from grB", dbname = "ngrams")


sqldf("insert into  tmp (word1,word2, freq) 
             select  word1,word2, sum(freq) as freq
             from gram2
             group by word1,word2",
      dbname = "ngrams")




i = 3

direction <- function(i)
{
  a <- if (i%%2==0) {list(big = "tmp", small = "gram2")} else {list(nig = "gram2", small = "tmp")}
  return(a)
}

table <- direction(6)


ptr  <- 1

toggle <- function(i)
{
  
  vals = c("gram2","nothing", "temp")
  ptr <<- ptr *(-1)
  vals[ptr + 2]
}
toggle()



table
swap(table)

```


```{r}

tic <- Sys.time()
FreqNgr <- parLapply(cl, 2:5, ngramize, Toks, FreqBig)
tocpar <- Sys.time() - tic
stopCluster(cl)

print(tocpar)
print(toc)

wat <- FreqNgr[[1]]

watt<- wat[wat$freq!=1,]

hist(log(watt$freq,100))

i = 5
lnums <- (i*sampSize+1):((i+1)*sampSize)
c(head(lnums),'...',tail(lnums))
Freq1gr2 <- sample_and_freq(currFile, lnums)
object.size(Freq1gr2)



Freq1gr<-merge(Freq1gr1,Freq1gr2, by.x = "words", by.y = "words", all =T, sort = T)
Freq1gr[is.na(Freq1gr)] <-0 #probably should do it with only numeric columns for speed
Freq1gr <- mutate(Freq1gr, freq = freq.x + freq.y)
Freq1gr <- Freq1gr[c("words","freq")]



```


```{r}


object.size(Freq1gr)/(object.size(Freq1gr1)+object.size(Freq1gr2))


wat1 <- head(Freq1gr1,50)
wat2 <- head(Freq1gr2,50)

watu<-merge(wat1,wat2, by.x = "words", by.y = "words", all =T, incomparables=NULL)


1+NA
c(wat1,wat2)

Lines <- get_lines('/home/michael/Studies/Coursera/10-Capstone/corpus/en_US/en_US.blogs.txt', 1:2)
head(Lines)
```


```{r save_ngrams, eval=FALSE, include=FALSE}
save(Dfm1gr,Dfm2gr,Dfm3gr,files,tfm, file = "123grams.rData")
```


```{r}
sqldf("select * from gram1  join words on (Id = word1)
where words == 'misanthrope'")
```



```{r}
wCols <- c("word1","word2")
FreqBig<-merge(grA,grB, by= wCols, all =T, sort = T)

#sum(!(grA$word  %in% grB$word))
dim(grA) + dim(grB) - dim(FreqBig)
  
```



```{r}

wat <- sqldf("SELECT name FROM sqlite_master WHERE type='table' AND name='gram7';", dbname = "ngrams")

sqldf("create table gram2 as select * from grA", dbname = "ngrams")

sqldf("create table tmp as select * from grB", dbname = "ngrams")
```



## the peculiar problem of saving 5grams in multiple files divided by first words.

```{r} 
# split range of Ids to given number of smaller ranges
uniqs <-sqldf("select max(Id) from words", dbname = "ngrams")
uniqs <- uniqs[[1]]
nboxes = 10
boxSz = floor(uniqs / nboxes) 
IdRngs <- c((0:(nboxes-1))*boxSz +1, uniqs)
IdRngs

i = 4
enrange <- function(i, grB, IdRngs)
{
  di <- IdRngs[i:(i+1)]
  sq <- seq(di[1],di[2])
  grC <- grB[grB$word1 %in% sq,]
  return(grC)
}  

wat <- enrange(i, grB, IdRngs)

watt<- lapply(1:nboxes, enrange, grB, IdRngs)
  
  
di <- IdRngs[i:(i+1)]
sq <- seq(di[1],di[2])

grC <- grB[sq,]

sum(grC)

range(grC$word1)
```


```{r}
a <- c("select w1.word as wd1, w2.word as wd2, gram2.freq
from gram2 join words w1 on (w1.id == gram2.word1)
                            join words w2 on (w2.id == gram2.word2) 
				where wd1 = 'forever' and wd2 = 'young'")
dbname<- "/home/michael/Studies/Coursera/10-Capstone/corpus/en_US/ngrams_big"
sqldf(a,dbname = dbname)
```



# Constructing the prediction model

```{r}
source('predict.R')
list.files()
#test constructQuery and predict.word functions
qtest5 <- constructQuery( search = c('said', 'first', 'quarter' ,'profit'),ngrO = 6)
writeLines(qtest5)
print(strrep("=",100))

qtest5m <- constructQuery( search = c('said', 'first', 'quarter' ,'profit'),ngrO = 5)
writeLines(qtest5m)
print(strrep("=",100))

qtest4 <- constructQuery( search = c('said','first', 'quarter' ,'profit'),ngrO = 4)
writeLines(qtest4)
print(strrep("=",100))

qtest4m <- constructQuery( search = c('first', 'quarter' ,'profit'),ngrO = 4)
writeLines(qtest4m)
print(strrep("=",100))

qtest3 <- constructQuery(search = c('quarter' ,'profit'),ngrO = 3)
writeLines(qtest3)


dbname <<- "/home/michael/Studies/Coursera/10-Capstone/corpus/en_US/ngrams"
x <- "said first quarter profit"
x <- "Hello, how are you"
predict.word(x)
predict.word("said")

```

# So the algorithm works as follows:
given: we have a string of 4 user input words: 'said', 'first', 'quarter' ,'profit'
1)search 5grams for which  those are 4 first words. Found:

  A)"said"	"first"	"quarter"	"profit"	"fell"  	"2.0"
  B)"said"	"first"	"quarter"	"profit"	"rose"  	"3.0"
  C)"said"	"first"	"quarter"	"profit"	"doubled"	"2.0"

2)search 4grams with exactly these words. Found:

  D)"said"	"first"	"quarter"	"profit"	          "14.0"

3)calculate alpha*(A,B,C)/D so that 
  P("fell"   |D) = 0.14
  P("rose"   |D) = 0.21
  P("doubled"|D) = 0.14

Move on to 4-grams. We have a string of 3 user input words: 'first', 'quarter' ,'profit'
4)search 4grams for which  those are 3 first words. Found:

  E)"first"	"quarter"	"profit"	"thanks"	    "2.0"
  F)"first"	"quarter"	"profit"	"million"	    "5.0"
  G)"first"	"quarter"	"profit"	"company"	    "2.0"
  H)"first"	"quarter"	"profit"	"large"	      "2.0"
  I)"first"	"quarter"	"profit"	"department"	"2.0"
  J)"first"	"quarter"	"profit"	"billion"	    "4.0"
  K)"first"	"quarter"	"profit"	"missed"	    "2.0"
  L)"first"	"quarter"	"profit"	"fell"	      "10.0"
  M)"first"	"quarter"	"profit"	"rose"	      "9.0"
  N)"first"	"quarter"	"profit"	"dropped"	    "3.0"
  O)"first"	"quarter"	"profit"	"compared"	  "2.0"
  P)"first"	"quarter"	"profit"	"jumped"	    "7.0"
  Q)"first"	"quarter"	"profit"	"gained"	    "2.0"
  R)"first"	"quarter"	"profit"	"doubled"	    "9.0"
  S)"first"	"quarter"	"profit"	"excluding"	  "2.0"
  T)"first"	"quarter"	"profit"	"soared"	    "4.0"


2)search 3grams with exactly these words. Found:

  U)"first"	"quarter"	"profit"	              "106.0"


3)calculate alpha*alpha*(E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T)/U so that 
  P("fell"   |D) = 0.14
  P("rose"   |D) = 0.21
  P("doubled"|D) = 0.14


```{r}
#searchTerm <-x
  x <- tolower(searchTerm)
  x <- corpus(x)
  x <- tokens(x, what = "word", remove_numbers = T, remove_punct = T, remove_symbols = T,
              remove_separators = T, remove_twitter = T, remove_hyphens = T, remove_url = T)
  x <-x[[1]]
  y = c("said", "rt", "quarter", "are")
  wat <- paste( "'", y, "'", sep="", collapse =",")
  ans <- sqldf(paste("select word from words where word in (",wat,")"),dbname = dbname )
  aa <- ans$word
  y[y %in% aa]
  
  vocabCheck(y)

```


```{r}
SSS <- sqldf("select pred, max(score) as score from SS group by pred order by score desc")
plot(SS$score)
plot(SSS$score)

system.time({sqldf("select word from words order by Id limit 10", dbname = dbname)})


```



## Predicting the quiz

```{r}
Quiz <- "The guy in front of me just bought a pound of bacon, a bouquet, and a case of
You're the reason why I smile everyday. Can you follow me please? It would mean the
Hey sunshine, can you follow me and make me the
Very early observations on the Bills game: Offense still struggling but the
Go on a romantic date at the
Well I'm pretty sure my granny has some old bagpipes in her garage I'll dust them off and be on my
Ohhhhh #PointBreak is on tomorrow. Love that film and haven't seen it in quite some
After the ice bucket challenge Louis will push his long wet hair out of his eyes with his little
Be grateful for the good times and keep the faith during the
If this isn't the cutest thing you've ever seen, then you must be"

Ans <- "pretzels
beer
cheese
soda
best
world
universe
most
bluest
happiest
smelliest
saddest
referees
defense
crowd
players
beach
movies
mall
grocery
phone
way
motorcycle
horse
weeks
years
thing
time
ears
eyes
toes
fingers
sad
bad
hard
worse
insane
callous
asleep
insensitive"

Ans <- strsplit(Ans,"\n")[[1]]
Ans <- matrix(Ans,nrow =4,ncol = 10)
colnames(Ans) <- 1:10

Quiz <- strsplit(Quiz,"\n")[[1]]


pred <- sapply(Quiz, predict.word, 7)
colnames(pred) <- 1:10
cm <- numeric(10)
for (i in 1:10)
{
  cm[i]<- sum(pred[,i] %in% Ans[,i])
}

cm
```






