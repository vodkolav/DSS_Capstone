---
title: "Week2"
author: "Michael Berger"
date: "10 November 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = '~/Studies/Coursera/10 - Capstone/corpus/en_US/')
library(readtext)
library(quanteda)
library(SnowballC)
library(ggplot2)
library(LaF)
library(dplyr)
library(tidyr)
library(plotly)
library(data.table)
library(hunspell)
```


```{r summaries, echo=FALSE, cache=T}

wc <- function(path)
{
    path = paste("'",path, "'", sep = "")
    w1 <- system(paste("wc -lwc",path), intern=TRUE)
    wsplit <- unlist(strsplit(w1," +"))
    wnum <-as.numeric(wsplit[2:4])
    return(data.frame('file' = wsplit[5], 'lines'=wnum[1], 'words'=wnum[2], 'chars'=wnum[3]))
}

folder = "/home/michael/Studies/Coursera/10-Capstone/corpus/en_US"

path <- paste(folder, currentFile, sep = "/")

filesinfo <- wc(path)

files = list.files(".", full.names = F)


lines = lapply(files, FUN = wc)# determine_nlines)

lala <- ldply(lines, rbind)

files <- data.frame(numlines = lines, stringsAsFactors = F)#, lines = as.vector(t(lns), mode = "any")

kable(files)
```


```{r}
enUS <- corpus(readtext('en_US.twitter.short.txt'))

enUSlines <- corpus_segment(enUS,pattern = "\n" )
docvars(enUSlines,'docnum')<- enUSlines$documents$`_segid`
rm( enUS)

#phrase <- "A computer once beat me at chess, but it was no match for me at kickboxing"
#phrase <- tokens(corpus("check out a movie"))
phrase <- tokens(corpus("rt"))
#cl <- textstat_collocations(phrase)
beat <- kwic(enUSlines,pattern = phrase, window = 50, case_insensitive = T, valuetype = "regex")
hasbeat <- beat[lengths(beat)>0]

bio <- enUSlines[15]
bio
texts
textstat_select(enUSlines, pattern = phrase,selection = "keep", valuetype = "fixed")
enUSlines[15]
```

```{r}
texts(enUSlines)[5]
```

```{r}
#enUS <- corpus(char_tolower(readNlines('en_US.twitter.txt', 40000)))

enUS <- corpus(char_tolower(get_lines('en_US.twitter.txt', 1:40000)))
lns <- get_lines('en_US.twitter.txt',1:2)# 40001:80000)
if (is.na(lns[1]))
  stop("shnitzl: couldn't read from file. probably path is wrong")
enUS80k <- corpus(char_tolower(get_lines('en_US.twitter.txt', 40001:80000))) 
head(texts(enUS80k))

profane <-char_tolower(get_lines('profanity_words.txt', 1:450))

# length(unique(profane))
tail(profane)
#enUSlines <- corpus_segment(enUS,pattern = "\n" )
#rm(enUS)
#save(enUSlines, file = "en_US.twitter.corpus.rds")

```


#Search for keywords in context
```{r}
#what = " +(rt) +"
what = "ぶ"
#what =  "#\\w+" 
#phrase <- tokens(corpus(what), what = "word")
context <- kwic(twitCorp ,pattern = what, window = 50, case_insensitive = T, valuetype = "regex")

txts <- twitCorp[context$docname]
htags <- context$keyword
head(htags, 20)
txts
```

```{r}
#words <- tokens(corpus(txts[1]))
words <- c("beer", "wiskey", "wine", "fsdngl","Taurus")
correct <- hunspell_check(words, dict = dictionary("en_US"))
print(correct)
```

```{r}
# this is a regex to find and substitute tokens which start with a number
withnums <- lvls[1:608]
#tail(lvls,40)

wn <- withnums[!grepl(pattern = "^(\\d+[,.]?)+", withnums)]
View(wn)

wihtoutnums <- gsub(pattern = "^(\\d+[,.]?)+", "", withnums)

View(cbind(withnums, wihtoutnums))
```



```{r}
# My attempt to create Twitter Hashtags segmentation algorithm 
source('~/Studies/Coursera/10 - Capstone/DSS_Capstone/Week2/hastags.r')
```



```{r}
enUStok <- tokens(enUS, what = "word", remove_numbers = T, remove_punct = T,
  remove_symbols = T, remove_separators = T,
  remove_twitter = T, remove_hyphens = T, remove_url = T,
  ngrams = 2:3)

```



```{r}
twitter_data <- stri_trans_general(twitter_data, "latin-ascii") #remove non-ASCII characters


# "(^|( +))(RT|rt)($| *:| +)"  regex to find RT (ReTweet) tag. Dont really need it. just remove_tokens("rt")


toks <- tokens(enUS80k, what = "word", remove_numbers = T, remove_punct = T,
  remove_symbols = T, remove_separators = T,
  remove_twitter = T, remove_hyphens = T, remove_url = T)
toks <- tokens_remove(toks, c(stopwords("english"), profane))
bigrams <- tokens_ngrams(toks, 2)


```

```{r}
rs<-colSums(Dfmtoks80k)
rs <- data.frame(word = names(rs), freq = rs)
rso <- rs[order(rs, decreasing = T )]


# with this you can check if there are some non-ascii chars remaining in dfm like chinese or russian 
lvls <- levels(rs$word)
from = 10000
lvls[from:(from+100)]
```

```{r}

Dfmtoks80k <- dfm(toks, tolower = TRUE, stem = F) # no stemming required i think
tf <- topfeatures(Dfmtoks80k,1000, scheme =  "docfreq")

tf <- data.frame(words = names(tf), freq = tf , stringsAsFactors = F)

Plot <- ggplot(head(tf,50), aes(x=reorder(words, freq), y=freq)) + 
        geom_bar(stat="Identity", fill='red') +
        coord_flip() #+ xlab(xlabel) + ylab(ylabel) + ggtitle(Gtitle)
Plot
```


```{r}
Dfm2grm80k <- dfm(bigrams, tolower = TRUE, stem = F) # no stemming required i think
tf <- topfeatures(bigramsDfm,1000, scheme =  "docfreq")

tf <- data.frame(words = names(tf), freq = tf , stringsAsFactors = F)

Plot <- ggplot(head(tf,50), aes(x=reorder(words, freq), y=freq)) + 
        geom_bar(stat="Identity", fill='red') +
        coord_flip() #+ xlab(xlabel) + ylab(ylabel) + ggtitle(Gtitle)
Plot
```


```{r}
ngrams <- tokens_ngrams(toks, n = 3)
trigramsDfm <- dfm(ngrams, tolower = TRUE, stem = F) # no stemming required i think
tf <- topfeatures(trigramsDfm,50, scheme =  "docfreq")

tf <- data.frame(words = names(tf), freq = tf , stringsAsFactors = F)

Plot <- ggplot(head(tf,50), aes(x=reorder(words, freq), y=freq)) + 
        geom_bar(stat="Identity", fill='red') +
        coord_flip() #+ xlab(xlabel) + ylab(ylabel) + ggtitle(Gtitle)
Plot
```


```{r}
ng <- rbind.dfm(Dfm2grm40k , Dfm2grm80k)
```

```{r}
# Bigram word combinations plot
Dfm2grm80k <- dfm(bigrams, tolower = TRUE, stem = F) # no stemming required i think
tf <- topfeatures(Dfm2grm80k,2000, scheme =  "docfreq")

tf <- data.frame(words = names(tf), freq = tf , stringsAsFactors = F)

# tfm <- mutate(tfm, word1 = strsplit(words,"_")[[1,]], word2 = strsplit(words,"_")[[2,]])
# word1 = strsplit(tfm$words,"_")
# word1[1][]
# tfm$word1 <- lapply(tf$words, FUN = function(wd) {strsplit(wd,"_")[[1]]}) 
# tfm$word2 <- lapply(tf$words, function(wd) {strsplit(wd,"_")[[2]]}) 
tfm <- tf %>% separate(col=words, sep = "_", into =c("word1", "word2"),remove =F)
#g<- ggplot(tfm, aes(word2,word1)) + geom_raster(aes(fill = freq)) + scale_fill_brewer(1)
  #scale_fill_viridis_c(option = "inferno")

g<- ggplot(tfm, aes(x=word2,y=word1, text = words, col = freq)) + geom_point(size = 1) + 
   scale_fill_viridis_c(aesthetics = "col",option = "D", direction = -1) + theme_classic()
ggplotly(g, tooltip = c("text","freq"))


length(unique(tfm$word1))
length(unique(tfm$word2))
```


```{r}

#An attempt to make a plot with less code using facet_grid. not really suitable for what I whant.  
tf <- topfeatures(Dfm1gr,30, scheme =  "docfreq")
tf1 <- data.frame(words = names(tf), freq = tf, gram = rep(1,length(tf)) , stringsAsFactors = F)

tf <- topfeatures(Dfm2gr,30, scheme =  "docfreq")
tf2 <- data.frame(words = names(tf), freq = tf , gram = rep(2,length(tf)) , stringsAsFactors = F)

tf <- topfeatures(Dfm3gr,30, scheme =  "docfreq")
tf3 <- data.frame(words = names(tf), freq = tf, gram = rep(3,length(tf))  , stringsAsFactors = F)

tf <- rbind(tf1, tf2,tf3)
                     
plt123grm <- ggplot(tf, aes(x=reorder(words, freq), y=freq)) + 
        geom_bar(stat="Identity", fill='red') + facet_grid(. ~ gram, scales = "free") +
        coord_flip() + xlab("1-grams") + ylab("Number of Apperarances") + ggtitle(" most frequent tokens") 

plt123grm

  
grid.arrange(plt1grm, plt2grm, plt3grm, ncol=3)

```

```{r}

ggplot(mlt, aes(x  = Var1, y = Var2)) + geom_raster(aes(fill = value), hjust=0.5,
vjust=0.5, interpolate=FALSE)
mlt <- melt(mt)
```



```{r fcm_plot}
if(F)
toks <- corpus_subset(enUS) %>%
    tokens(remove_punct = TRUE) %>%
    tokens_tolower() %>%
    tokens_remove(stopwords("english"), padding = FALSE)
myfcm <- fcm(toks, context = "window", tri = FALSE)
feat <- names(topfeatures(myfcm, 30))
fcm_select(myfcm, feat, verbose = FALSE) %>%
    textplot_network(min_freq = 0.5)
```



# should use benchmarking (https://github.com/hfoffani/dsci-benchmark)
# should use LaF package (https://rdrr.io/cran/LaF/man/)  (Fast Access to Large ASCII Files)



# get number of lines. there is a  determine_nlines function in the LaF package and its 52 times faster.
```{r }
nlines <- function(filename )
{
  tic <- Sys.time()
  testcon <- file(filename,open="r")
  readsizeof <- 20000
  answer <- 0
  while((linesread <- length(readLines(testcon,readsizeof))) > 0 ) 
  {
    answer <- answer+linesread 
  }
  close(testcon)
  toc <- Sys.time() - tic
  print(toc)
  answer
}

nl <- nlines("en_US.twitter.txt")

tic <- Sys.time()
dnl <- determine_nlines("en_US.twitter.txt")
toc <- Sys.time() - tic
print(toc)
9.9/0.19

```

```{r}
timeit
```


```{r}


readNlines <- function(filename, N)
{
  tic <- Sys.time()
  #N <- 40000
  #filename <- "en_US.twitter.txt"
  conn <- file(filename,open="r")
  lines <- readLines(conn,N)
  close(conn)
  toc <- Sys.time() - tic
  toc
  lines
}
#test
t <-readNlines("en_US.twitter.txt", 13)
```

```{r}
readRangelines <- function(filename, from = 0 , to, num)
{
  #N <- 40000
  #filename <- "en_US.twitter.txt"
  conn <- file(filename,open="r")
  if(from!=0)
    devnull <- readLines(conn,from-1)
  if(missing(num)){num <- to-from}
  if(from > to) {stop("You think you are funny?")}
  lines <- readLines(conn,num)
  close(conn)
  lines
}
#test
readRangelines("en_US.twitter.txt",7,4)
```





