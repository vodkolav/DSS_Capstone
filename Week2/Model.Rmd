---
title: "Milestone Report"
author: "Michael Berger"
date: "2 December 2018"
output: 
  html_document: 
    fig_height: 7
    fig_width: 11
---

```{r setup, include=FALSE}
#knitr::opts_chunk$set(include = FALSE)
#knitr::opts_knit$set(root.dir = '~/Studies/Coursera/10-Capstone/DSS_Capstone/')
data.dir <<- "/home/michael/Studies/Coursera/10-Capstone/corpus/en_US/"

 library(knitr)
# library(readtext)
 library(quanteda)
# library(SnowballC)
 library(ggplot2)
 library(LaF)
 library(plyr)
 library(dplyr)
 library(tidyr)
# library(plotly)
 library(data.table)
# library(stringi)
# library(hunspell)
# library(gridExtra)
# library(viridis)
 library(RColorBrewer)
library(snow)
library(profvis)
library(sqldf)
```

    
```{r summaries, echo=FALSE, cache=T}

wc <- function(path)
{
    size <- round(file.size(path)/(2^20))
    path = paste("'",path, "'", sep = "")
    w1 <- system(paste("wc -lwc",path), intern=TRUE)
    
    wsplit <- unlist(strsplit(w1," +"))
    wnum <-as.numeric(wsplit[2:4])
    return(data.frame('file' = wsplit[5], 'size,MiB' = size, 'lines'=wnum[1], 'words'=wnum[2], 'chars'=wnum[3], stringsAsFactors = F))
}

files = list.files(".",pattern = "^en", full.names = F)

files = lapply(files, FUN = wc)# determine_nlines)

#filesbk <- files #files <- filesbk  , stringsAsFactors = F

files <- data.frame(ldply(files, rbind))
rownames(files) = files$file

files[nrow(files)+1,2:5] <- apply(files[,2:5],2,sum)
files[nrow(files),1] <- "Total"

files$wordsPerLine <- round(files$words/files$lines)
files$charsPerLine <- round(files$chars/files$lines)

save(files, file = 'filesInfo.rData')

kable(files, row.names = F,format.args = list(decimal.mark = ".", big.mark = ","))
```
 



## First pass on the data
Only create index of all unique words to get Ids for the ngrams words.
This bit of code is highly diplicate, SO I need to unite it with same function for n>1 grams to shrink it.
```{r all_processing_functions}

profane <- paste(data.dir,'profanity_words.txt', sep = "")
l <- determine_nlines(profane) #read profanity vocabulary and convert to lower
profane <-char_tolower(get_lines(profane, 1:l))
profane <<- profane[!is.na(profane)]


load(file = paste(data.dir,'filesInfo.rData', sep = "")) #load information about files: specifically number of lines 

#User Parameters:

#currFile <- 'en_US.blogs.txt'
# sampSize = 5e4  #1e4 # how much lines to sample in a chunk
# ngrO = 1 #:5 # what Orders of ngrams you require




sample_and_toks <- function(filepath ,line_numbers)
{
  
  if (is.na(get_lines(filepath,1:2)[1])) # check that I can read from file
  {
    print(paste("can't read from file", filepath))
    stop()
  }
  
  Lines <- get_lines(filepath, line_numbers) #sample some lines from a file
  Encoding(Lines) <- "latin1"  #remove non-ascii chars 
  Lines <- iconv(Lines, "latin1", "ASCII", sub="")
  Lines <- gsub('_+', ' ', Lines, perl=T) #replace all underscores (including multiple _____) with whitespace
  Corp <- corpus(char_tolower(Lines)) # everything to lowerCase and create corpus
  
  Toks<- tokens(Corp, what = "word", remove_numbers = T, remove_punct = T, remove_symbols = T,
                remove_separators = T, remove_twitter = T, remove_hyphens = T, remove_url = T)
  rm(Lines,Corp)
  Toks <- tokens_remove(Toks, c(stopwords("english"), profane, "rt"))# also remove ReTweet tag
  
  return(Toks) #(cS)# 
}  

ngramize <-function (n, Toks, FreqBig)
{
  library(quanteda)
  library(data.table)
  library(dplyr)
  ngrams <- tokens_ngrams(Toks, n)
  DfmNgr <- dfm(ngrams, tolower = F)
  cS<-sort(colSums(DfmNgr), decreasing = T)
  FreqThis <- data.table(word = names(cS), freq = cS , stringsAsFactors = F, key = "word")
  #FreqThis <- FreqNgr1$gr2
  
  #wordCols <- paste("word", 1:n, sep = '')
  wordCols <-"word"
  
  # FreqThis <- FreqThis %>% separate(col=word, sep = "_", into = wordCols ,remove =T)
 
  #find ids of each first word in parent table 
  # word1 <- sqldf("select Id, word from words, FreqThis where (words.word == FreqThis.word1)", dbname = "ngrams")
  # FreqThis$word1 <- word1$Id
  # 
  # word2 <- sqldf("select Id, word from words, FreqThis where (words.word == FreqThis.word2)", dbname = "ngrams")
  # FreqThis$word2 <- word2$Id
  
  # for (i in 1:n)
  # {
  #   wordi <- sqldf(paste("select Id, word from words, FreqThis where (words.word == FreqThis.word",i, ")", 
  #                        sep = ""), dbname ="ngrams")
  #   FreqThis[,(wordCols[i]) := wordi$Id]
  # }
  #FreqBig<-merge(FreqBig[[paste("gr",n,sep = '')]],FreqThis, by.x = wordCols, by.y = wordCols, all =T, sort = T)
  FreqBig<-merge(FreqBig,FreqThis, by.x = wordCols, by.y = wordCols, all =T, sort = T)
  FreqBig[is.na(FreqBig)] <-0 #probably should do it with only numeric columns for speed
  FreqBig <- mutate(FreqBig, freq = freq.x + freq.y)
  
  return(select(FreqBig, -c("freq.x", "freq.y")))
}





#prof = NULL
#prof <- profvis({


ngrams <- function(path, sampSize = 5e4, ngrO = 1, env = "test" )
{
  tic <- Sys.time()
  obs <- numeric()
  elapsed <- Sys.time()
  
  #A list of data.tables each of which holds ngrams of its order
  a <- list(data.table(word = numeric(), freq = numeric(), stringsAsFactors = F, key = "word"))
  FreqNgr <- rep(a, length(ngrO))
  
  # wordCols <- paste("word", 1:ngrO, sep = '')
  # a <- data.table(freq = numeric()) 
  # a[,(wordCols) := numeric(ngrO), by=freq]
  # # FreqNgr <- list(a) # for compatibility I'll leave it as list of data.tables, but only one d.t will be there
  # # names(FreqNgr) <- paste("gr",ngrO,sep = '') #stamp a names such as 'gr2', 'gr3' etc to each data.table in a list 
  # FreqNgr <- a
  
  
  for (currFile in files[-4,]$file)
  {
    #Derived parameters
    filename <- paste(path,  currFile, sep = '')
    
    sampSize <- ifelse(env=="test", 1e2, sampSize )
    nslices = floor(files[currFile,]$lines / sampSize) #upper boundary for processing loop
    #rng = (nslices-1):nslices # 10 # for quick tests 
    #rng = 0:nslices # 10 # for full file run. must start at zero
    
    rndslices <- ceiling(runif(1,0,nslices)) # smaple random place in data for testing
    rng <- if(env=="test") (rndslices-1):rndslices else 0:nslices 
    
    
    #cl <- makeSOCKcluster(rep("localhost", length(ngO)))
    
    for (i  in rng) # 900)#
    {
      lnums <- (i*sampSize+1):((i+1)*sampSize)
      
      toc <-difftime(Sys.time(), tic, units = "mins")
      toc <- paste("| ", sprintf("%.1f", toc ), " ", attr(toc, "units"), "elapsed")
      # substr(filename,56,nchar(filename)) #56 is lenth of path
      print(paste("processing file ", currFile , "  lines ", 
                  lnums[1], " - ", tail(lnums,1),toc))
      
      Toks <- sample_and_toks(filename, lnums)
      FreqNgr <- ngramize(ngrO, Toks, FreqNgr)
      
      #FreqNgr <- parLapply(cl, ngO, ngramize, Toks, FreqNgr)
      
      ob <-dim(FreqNgr)[1]
      obs <- c(obs, ob)
      print(paste(ob, "unique tokens| ", object.size(FreqNgr), " bytes"))
      elapsed <- c(elapsed, difftime(Sys.time(), tic, units = "mins"))
    }
    #stopCluster(cl)
    #}, prof_output = ".")
    
    
    #Freq1grBlogs <- FreqNgr
    #
    #FreqNgr["gr2"]
    #profvis(prof_input = "/home/michael/Studies/Coursera/10-Capstone/corpus/en_US/file4fce8ccb11c.Rprof")
    
  }
  save(FreqNgr,obs, file = paste(path, ngrO, "grams." ,env, ".allFiles.rData", sep = ''))
  return(list(FreqNgr,obs))
}

# tmp <- ngrams(path, sampSize = 5e2, ngrO = 2, env = "test" )
# FreqNgr <- tmp[[1]]
# obs <- tmp[[2]]
# plot(obs, type = 'l')

```

TODO: make Id primary key and unique 
      or
      make both index words_Id and words_word

```{r} 
#this creates table of all unique words and their IDs + table of 1-grams 

FreqNgr <- ngrams(data.dir, sampSize = 1e5, ngrO = 1, env = "run")

gr1 <- FreqNgr[[1]]
gr1 <- gr1[order(gr1$freq, decreasing = T),]
gr1$Id <- 1:dim(gr1)[1]

sqldf("attach 'ngrams' as new") 

#names(gr1)[1]<- "word"

sqldf("create table words as select Id, word from gr1 ", dbname = "ngrams")
sqldf("create table gram1 as select Id as word1, freq from gr1 ", dbname = "ngrams")

sqldf("CREATE UNIQUE INDEX `Words_word_index` ON `words` (`word` ASC)", dbname = "ngrams")
sqldf("CREATE UNIQUE INDEX `Words_Id_index` ON `words` (`Id` ASC)", dbname = "ngrams")

sqldf("CREATE INDEX `gram1_index` ON `gram1` (`word1` ASC)", dbname = "ngrams")
  
sqldf("CREATE VIEW `select some 1-gram` AS select * from gram1  join words on (Id = word1)
      where word == 'disrespect'", dbname = "ngrams")
sqldf("CREATE VIEW `top 50 1grams` AS select  * from gram1  join words on (Id = word1) 
      limit 50", dbname = "ngrams")
```



So the options to improve performance are:
1) at every sample drop all the n-grams which have frequency 1.  but some ngrams with whole corpus frequency higher than one will be dropped just because they appear only once in some single  samples.
2) somehow merge all the samples data in memory and then drop ngrams with frequency 1
3) store the big ngrams tables in SQL database and update it one by one as new samples are processed. 
4) store only unique words (1-grams basically) and for all other grams - tables with indices of words. 
  -Scan all the files for unique words first so I will have indices for all the words in data 
  -Construct all n>1grams based on it. 
  -To merge data from separate samples I use SQL's SUM() function with group by aggreagate.


## Testing environment
test everything runs smoothly before investing in long process of running all the data  
```{r}
# tmp <- ngrams(path, sampSize = 1e4, ngrO = 2, env = "test")
# grA <- tmp[[1]]
# 
# tmp <- ngrams(path, sampSize = 1e4, ngrO = 2, env = "test")
# grB <- tmp[[1]]

source('Week2/debug.R')

result1 <- ngrams(data.dir, sampSize = 1e4, ngrO = 2, env = "test")

# "select count(word1) from gram2"

result2 <- ngrams(data.dir, sampSize = 1e4, ngrO = 2, env = "test")

sqldf("select w1.word as wd1, w2.word as wd2, gram2.freq
               from gram2 join words w1 on (w1.id == gram2.word1)
                            join words w2 on (w2.id == gram2.word2) 
				where wd1 = 'years' and wd2 = 'ago'", dbname = "ngrams" )

```

##Running environment
Actually Creating all the 2 to 5 grams. Careful! takes several hours to run and enslaves your PC.
So it's set to eval= False so that you dont run it accidentally when knitting
To run, set eval=True

```{r eval=FALSE, include=FALSE}

results <- ngrams(data.dir, sampSize = 1e5, ngrO = 2, env = "run")
save(results, file = "3grams.perf.graph.rdata")

results <- ngrams(data.dir, sampSize = 1e5, ngrO = 3, env = "run")
save(results, file = "3grams.perf.graph.rdata")

results <- ngrams(data.dir, sampSize = 1e5, ngrO = 4, env = "run")
save(results, file = "4grams.perf.graph.rdata")

results <- ngrams(data.dir, sampSize = 1e5, ngrO = 5, env = "run")
save(results, file = "5grams.perf.graph.rdata")

```

## plot performance metrics
```{r}
time <- results[[1]]
ngrs <- results[[2]]
c <- cbind(time,ngrs)
plot(c, type = 'l')
```


## deflating database
removing all records where freq = 1 and decreasing database's file size on disk
```{r}
tables <- paste("gram", 3:5, sep = "") # I'll leave 1 and 2 grams with freq = 1 
deleteQrs <- paste("delete from", tables, "where freq = 1")

sqldf(deleteQrs, dbname = "ngrams")

sqldf("vacuum", dbname = "ngrams")

```

##Create indices
```{r}
i <- 2:5

idxQrs <- paste("CREATE INDEX `gram5_",i, "index` ON `gram5` (`word",i,"` ASC)" , sep = "")
idxQrs[1]
sqldf(idxQrs, dbname = "ngrams")

```


















