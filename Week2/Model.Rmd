---
title: "Milestone Report"
author: "Michael Berger"
date: "2 December 2018"
output: 
  html_document: 
    fig_height: 7
    fig_width: 11
---

```{r setup, include=FALSE}
#knitr::opts_chunk$set(include = FALSE)
#knitr::opts_knit$set(root.dir = '~/Studies/Coursera/10-Capstone/DSS_Capstone/')

library(knitr)
library(quanteda)
library(ggplot2)
library(LaF)
library(plyr)
library(dplyr)
library(tidyr) 
library(data.table)
library(RColorBrewer)
library(snow)
library(sqldf)
library(plotly)
library(profvis)

#because I always mistype V in View
vu <- function(x){View(x)} 

# useful for concatenating parts of filepath
`%/%` = function(e1,e2) return(paste(e1,e2, sep = "/"))

 data.dir <- "/home/michael/Studies/Coursera/10-Capstone/corpus/en_US"
 
# db which has:1 to 5 grams, stopwords remain , freq=1 filtered out, indices created, 746 MB
 dbname <- data.dir %/% "ngrams"

# db which has:1 to 5 grams, stopwords removed, freq=1 filtered out, indices created, 343 MB
#dbname <- "/media/michael/SSData/Studies/Data Science Capstone/ngrams (backup_1to5grams_filtered_indices)"

# db which has:1 to 5 grams, stopwords remain, freq=1 remain, indices created, 7.8 GB
#dbname <- "/media/michael/SSData/Studies/Data Science Capstone/ngrams (all_grams with_stopwords)"

# db which has:1 to 5 grams, stopwords removed, freq=1 remain, indices created, 6 GB
#dbname <- "/media/michael/SSData/Studies/Data Science Capstone/ngrams (backup_1to5grams_with_freq1)"



print(paste("ngrams database file: ",dbname))
list.files()
source('train.R')
source('../SlothKey/predict.R')
load('../SlothKey/profane.rda')
# library(SnowballC)
# library(stringi)
# library(hunspell)
# library(gridExtra)
# library(viridis)
# library(readtext)

```

Careful!     
```{r eval=FALSE, include=FALSE}
if(F){rm(list = ls())}
```
    
    
    
```{r summaries, echo=FALSE, cache=T}

wc <- function(path)
{
    size <- round(file.size(path)/(2^20))
    path = paste("'",path, "'", sep = "")
    w1 <- system(paste("wc -lwc",path), intern=TRUE)
    
    wsplit <- unlist(strsplit(w1," +"))
    wnum <-as.numeric(wsplit[2:4])
    return(data.frame('file' = wsplit[5], 'size,MiB' = size, 'lines'=wnum[1], 'words'=wnum[2], 'chars'=wnum[3], stringsAsFactors = F))
}

files = list.files(".",pattern = "^en", full.names = F)

files = lapply(files, FUN = wc)# determine_nlines)

#filesbk <- files #files <- filesbk  , stringsAsFactors = F

files <- data.frame(ldply(files, rbind))
rownames(files) = files$file

files[nrow(files)+1,2:5] <- apply(files[,2:5],2,sum)
files[nrow(files),1] <- "Total"

files$wordsPerLine <- round(files$words/files$lines)
files$charsPerLine <- round(files$chars/files$lines)

save(files, file = 'filesInfo.rData')

kable(files, row.names = F,format.args = list(decimal.mark = ".", big.mark = ","))
```
 



## First pass on the data
Only create index of all unique words to get Ids for the ngrams words.

```{r all_processing_functions}

mergeVocabs <-function (FreqBig, FreqThis)
{  
  #wordCols <- paste("word", 1:n, sep = '')
  wordCols <-"word"
  
  FreqBig<-merge(FreqBig,FreqThis, by.x = wordCols, by.y = wordCols, all =T, sort = T)
  FreqBig[is.na(FreqBig)] <-0 #probably should do it with only numeric columns for speed
  FreqBig <- mutate(FreqBig, freq = freq.x + freq.y)
  cnt <- nrow(FreqBig)
  obs <<- c(obs, cnt) # save number of unique ngrams for the performance report
  print(paste(cnt, " Unique words in vocabulary" ))
  return(select(FreqBig, -c("freq.x", "freq.y")))
}


buildVocabulary <- function(path, sampSize = 5e4, ngrO = 1, mode = "test" )
{
  if (mode=="test"){print("running in testing mode...")}
  tic <- Sys.time()
  obs <<- numeric()
  elapsed <- Sys.time()
  sampSize <- ifelse(mode=="test", 1e2, sampSize )
  
  FreqNgr<- list(data.table(word = numeric(), freq = numeric(), stringsAsFactors = F, key = "word"))
  
  for (currFile in files[-nrow(files),]$file)
  {
    #Derived parameters
    filename <- paste(path,  currFile, sep = '')
    nslices <- slices(files[currFile,], sampSize) #upper boundary for processing loop
    rndslices <- ceiling(runif(1,0,nslices)) # smaple random place in data for testing
    rng <- if(mode=="test") (rndslices-2):rndslices else 0:nslices 
    
    for (i  in rng) # 900)#
    {
      lnums <- (i*sampSize+1):((i+1)*sampSize)
      toc <-difftime(Sys.time(), tic, units = "mins")
      toc <- paste("| ", sprintf("%.1f", toc ), " ", attr(toc, "units"), "elapsed")
      print(paste("processing file ", currFile , " lines ", lnums[1], " - ", tail(lnums,1),toc))
      
      # The actual processig steps
      Lines   <- sample_text(filename, lnums) 
      Toks    <- tokenize(Lines, ngrO)
      FreqThis <- Freqs(Toks)
      rm(Toks,Lines)
      FreqNgr <- mergeVocabs(FreqNgr, FreqThis)
      
      elapsed <- c(elapsed, difftime(Sys.time(), tic, units = "mins"))
    }
  }
  #save(FreqNgr,obs, file = paste(path, ngrO, "grams." ,mode, ".allFiles.rData", sep = ''))
  return(list(FreqNgr = FreqNgr,perf = data.frame(elapsed = unclass(elapsed), objects =obs, ngrO = rep(ngrO, length(obs)))))
}


```

TODO: make Id primary key and unique 
      or
      make both index words_Id and words_word

```{r}
result <- buildVocabulary(data.dir, sampSize = 1e2, ngrO = 1, mode = "test")
```


```{r} 
#this creates table of all unique words (vocabulary) and their IDs + table of 1-grams 

result <- buildVocabulary(data.dir, sampSize = 1e5, ngrO = 1, mode = "run")

results01 <- result$perf
save(results0, file = paste(data.dir,"perf", '0grams.graph.rdata',sep = "/"))


gr1 <- result[[1]]
gr1 <- gr1[order(gr1$freq, decreasing = T),]
gr1$Id <- 1:dim(gr1)[1]

sqldf(paste("attach '" ,dbname,"' as new", sep = ""))

#names(gr1)[1]<- "word"

sqldf("create table words as select Id, word from gr1 ", dbname = dbname)
sqldf("create table gram1 as select Id as word1, freq from gr1 ", dbname = dbname)

sqldf("CREATE UNIQUE INDEX `Words_word_index` ON `words` (`word` ASC)", dbname = dbname)
sqldf("CREATE UNIQUE INDEX `Words_Id_index` ON `words` (`Id` ASC)", dbname = dbname)
  
sqldf("CREATE VIEW `select some 1-gram` AS select * from gram1  join words on (Id = word1)
      where word == 'disrespect'", dbname = dbname)
sqldf("CREATE VIEW `top 50 1grams` AS select  * from gram1  join words on (Id = word1) 
      limit 50", dbname = dbname)
```



So the options to improve performance are:
1) at every sample drop all the n-grams which have frequency 1.  but some ngrams with whole corpus frequency higher than one will be dropped just because they appear only once in some single  samples.
2) somehow merge all the samples data in memory and then drop ngrams with frequency 1
3) store the big ngrams tables in SQL database and update it one by one as new samples are processed. 
4) store only unique words (1-grams basically) and for all other grams - tables with indices of words. 
  -Scan all the files for unique words first so I will have indices for all the words in data 
  -Construct all n>1grams based on it. 
  -To merge data from separate samples I use SQL's SUM() function with group by aggreagate.


## Testing environment
test everything runs smoothly before investing in long process of running all the data  
```{r}

results2 <- train.word(data.dir, sampSize = 1e2, ngrO = 2, mode = "test")

results3 <- train.word(data.dir, sampSize = 1e2, ngrO = 3, mode = "test")

results4 <- train.word(data.dir, sampSize = 1e2, ngrO = 4, mode = "test")

results5 <- train.word(data.dir, sampSize = 1e2, ngrO = 5, mode = "test")

```

##Running environment
Actually Creating all the 2 to 5 grams. 
Careful! takes several hours to run and enslaves your PC.
It's set to eval=False so that you dont run it accidentally when knitting
To run, set eval=True

```{r eval=FALSE, include=FALSE}

results2 <- train.word(data.dir, sampSize = 1e5, ngrO = 2, mode = "run")
save(results2, file = paste(data.dir, "perf", "2grams.graph.rdata",sep = "/"))

results3 <- train.word(data.dir, sampSize = 1e5, ngrO = 3, mode = "run")
save(results3, file = paste(data.dir, "perf", "3grams.graph.rdata",sep = "/"))

results4 <- train.word(data.dir, sampSize = 1e5, ngrO = 4, mode = "run")
save(results4, file = paste(data.dir, "perf", "4grams.graph.rdata",sep = "/"))

results5 <- train.word(data.dir, sampSize = 1e5, ngrO = 5, mode = "run")
save(results5, file = paste(data.dir, "perf", "5grams.graph.rdata",sep = "/"))

```

## plot performance metrics
```{r}
# results0 <-load(data.dir %/% "perf/1grams.perf.graph.rData")
# results0 <- results0[[1]]
# vu(results0)
results <- rbind(results2,results3,results4, results5)
results$ngrO <- factor(results$ngrO)
save(results, file = paste(data.dir, "allGrams.perf.graph.rdata",sep = "/"))

g <- ggplot(data = results) + geom_line(aes(x= elapsed, y = objects, color = ngrO))
g

totaltime <- tail(results2$elapsed,1) +  tail(results3$elapsed,1) + tail(results4$elapsed,1) + tail(results5$elapsed,1) 

g1 <- ggplot(data = )
#ggplotly(g)

```


## deflating database
removing all records where freq = 1 and decreasing database's file size on disk
```{r}
ptables <- paste("gram", 2:5, sep = "") # 1 grams with freq = 1 will not be removed
deleteQrs <- paste("delete from", ptables, "where freq = 1")

sqldf(deleteQrs, dbname = dbname)

sqldf("vacuum", dbname = dbname)

```

##Create indices (dont forget for all the tables)
```{r}
ngrO<-5
i <- 1:ngrO
#example:  c("CREATE INDEX `gram5_1to5index` ON `gram5` (`word1` DESC,`word2` DESC,`word3` DESC,`word4` DESC,`word5` DESC)")

pwords <- function(i){paste("`word",1:i,"` ASC", sep = "",collapse=", ")}

idxQrs <- paste("CREATE UNIQUE INDEX `gram",i,"_1to",i, "index` ON `gram",i,"` (",lapply(i, pwords),")" , sep = "")

writeLines(paste(idxQrs, collapse = ";\n"))

sqldf(idxQrs, dbname = dbname)

```

#Prediction 

## Test some predictions
```{r}



x <- "said first quarter profit"
x <- "Hello, how are you"
x <- "technical discussions by people that are serious about protecting the"
x <- "I like how the same people are in almost all of Adam Sandler's" 
pred <- predict.word(x, 50)

"us" %in% pred

vu(pred)
predict.word("")
profvis({predict.word(x)})
system.time(y <- predict.word(x))
y

profvis({t <- tokenize(x)})


```
 The predicting model benchmarking


```{r}
#options(warn=1)
#test.data.size <-7
#lnums <- 1:7
lnums <- 347:348
source("../dsci-benchmark/benchmark.R")
                                                        ##,'blogs' = blogs
profvis({bench <- benchmark(predict.word, sent.list = li+st('tweets' = tweets), ext.output = F)})

```





Very (maybe too) good result
Overall top-3 score:     26.78 %
Overall top-1 precision: 24.22 %
Overall top-3 precision: 28.99 %
Average runtime:         112.05 msec
Number of predictions:   10079
Total memory used:       5.27 MB

Dataset details
 Dataset "blogs" (1 lines, 20 words, hash a0a8c15f90732b52f7e6eb85d7c15878bdb924a69866718e987cb6e256776cff)
  Score: 31.58 %, Top-1 precision: 31.58 %, Top-3 precision: 31.58 %
 Dataset "tweets" (860 lines, 10822 words, hash 38bd3d1edf091d9b58da4701030ca08a464904e731281220ced2bc7514b81a74)
  Score: 21.98 %, Top-1 precision: 16.86 %, Top-3 precision: 26.39 %





test.data.size <-7
Overall top-3 score:     19.84 %
Overall top-1 precision: 15.16 %
Overall top-3 precision: 23.89 %
Average runtime:         96.28 msec
Number of predictions:   323
Total memory used:       2.20 MB

Dataset details
 Dataset "blogs" (7 lines, 219 words, hash 7826fbcd874bcb695defcfa68c918e902cfb463d481de5e289c242ae117cbbc3)
  Score: 17.45 %, Top-1 precision: 13.21 %, Top-3 precision: 20.75 %
 Dataset "tweets" (8 lines, 117 words, hash a9f3535627edacf86ee788cf684f1be505ed0fd72d213defe35c33ac3f80c735)
  Score: 22.22 %, Top-1 precision: 17.12 %, Top-3 precision: 27.03 %







